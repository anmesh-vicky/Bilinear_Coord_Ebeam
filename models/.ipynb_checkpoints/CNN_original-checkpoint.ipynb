{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmednhL2tep7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.signal import hamming\n",
    "\n",
    "\n",
    "\"\"\"This function creates the lattice points for the all the images in a single go\n",
    "and stacks them in 3D\"\"\"\n",
    "def lattice(parms,nxx=20, nyy=20):\n",
    "  a1 = parms[0]\n",
    "  a2 = parms[1]\n",
    "  phi = parms[2]\n",
    "  \n",
    "  nx,ny = np.meshgrid(np.arange(nxx), np.arange(nyy))\n",
    "  values = (nx.ravel(),ny.ravel())\n",
    "  values = np.array(values).T\n",
    "  nxx = values[:,0]\n",
    "  nyy = values[:,1]\n",
    "  j = a1.shape[0]\n",
    "  atom_pos = np.zeros((j,400,2))\n",
    "  for i in range(0,j):\n",
    "\n",
    "      x_ind = nxx * a1[i] + nyy * a2[i] * np.cos(phi[i])\n",
    "      y_ind = nyy * a2[i] * np.sin(phi[i])\n",
    "      atom_pos[i,:,0] = x_ind\n",
    "      atom_pos[i,:,1] = y_ind\n",
    "  \n",
    "  return atom_pos\n",
    "\n",
    "\n",
    "\"\"\"This function distorts the positions of all the atoms in the lattice with\n",
    "standard deviation 30% of the lattice parameter a1\"\"\"\n",
    "def distortions(atom_pos, parms):\n",
    "  j = atom_pos.shape[0]\n",
    "  a1 = parms[0]\n",
    "  a2 = parms[1]\n",
    "  atom_pos_dis = np.zeros((j,400,2))\n",
    "  for i in range(j):\n",
    "    x_dis = np.random.normal(loc = 0.0, scale = 0.03*a1[i] ,size = [400,1])\n",
    "    y_dis = np.random.normal(loc = 0.0, scale = 0.03*a2[i] ,size = [400,1])\n",
    "\n",
    "    dis = np.concatenate((x_dis,y_dis),axis=1)\n",
    "    atom_pos_dis[i] = atom_pos[i] + dis\n",
    "  \n",
    "  return atom_pos_dis\n",
    "\n",
    "\n",
    "def atom_to_image(atom_pos_dis,img_dim = 1024):\n",
    "  j = atom_pos_dis.shape[0]\n",
    "  image_atoms = np.zeros([j,img_dim,img_dim])\n",
    "    \n",
    "  for i in range(0,j):\n",
    "      max_x = np.max(atom_pos_dis[i,:,0])\n",
    "      max_y = np.max(atom_pos_dis[i,:,1])\n",
    "\n",
    "      min_x = np.min(atom_pos_dis[i,:,0])\n",
    "      min_y = np.min(atom_pos_dis[i,:,1])\n",
    "\n",
    "\n",
    "\n",
    "      x1,y1 = atom_pos_dis[i,:,0], atom_pos_dis[i,:,1]\n",
    "      x_img = ((x1 - min_x)/(max_x - min_x) * (img_dim-1))       \n",
    "      y_img = ((y1-min_y)/(max_y - min_y) * (img_dim-1))\n",
    "\n",
    "      x_img = x_img.astype(int)\n",
    "      y_img = y_img.astype(int)  \n",
    "      image_atoms[i,x_img, y_img]=1E6           \n",
    "\n",
    "\n",
    "  return image_atoms\n",
    "\n",
    "def convolve_atomic_image(image_atoms, sigma = 6):\n",
    "  j = image_atoms.shape[0]\n",
    "  con_img = np.zeros([j,1024,1024])\n",
    "  for i in range(0,j):\n",
    "      con_img[i] = gaussian_filter(image_atoms[i],sigma,order = 0)\n",
    "  return con_img\n",
    "\n",
    "def crop(convolved_img):\n",
    "    crop_img = convolved_img[:,350:700,350:700]\n",
    "    \n",
    "    \n",
    "    return crop_img\n",
    "\n",
    "def fft(crop_img):\n",
    "    img_ffts=[]\n",
    "    fft_win_size =64\n",
    "    j = crop_img.shape[0]\n",
    "    for j in range(0,j):\n",
    "        \n",
    "        n = crop_img.shape[1]\n",
    "        h = hamming(n) \n",
    "        ham2d = np.sqrt(np.outer(h,h))\n",
    "        img_windowed = np.copy(crop_img[j])\n",
    "        img_windowed *= ham2d \n",
    "        img_fft = np.fft.fftshift(np.fft.fft2(img_windowed))\n",
    "        img_fft = img_fft[crop_img.shape[1]//2 - fft_win_size:crop_img.shape[1]//2+fft_win_size,\n",
    "                                     crop_img.shape[1]//2 - fft_win_size:crop_img.shape[1]//2+fft_win_size]\n",
    "        img_ffts.append(img_fft)\n",
    "        \n",
    "    return np.array(np.sqrt(np.abs(img_ffts)))\n",
    "  \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-qs1Btfx2tl"
   },
   "outputs": [],
   "source": [
    "\"\"\"Debugged\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\"\"\"Generates 400 lattice parameters for each type of bravis lattice,\n",
    "labels and shuffles them and output 100 data points when called\"\"\"\n",
    "\n",
    "def generate():\n",
    "#oblique   a1 != a2, phi!=90 Label = 0\n",
    "    bond_len = np.random.uniform(low = [0.8,0.8], high = [2.0,2.2],size = [800,2])\n",
    "    diff = np.abs(((bond_len[:,0]-bond_len[:,1])*100)/bond_len[:,1])\n",
    "\n",
    "    bond_len = bond_len[diff > 20.0,:]\n",
    "    bond_len = bond_len[0:400,:]\n",
    "    phi = np.zeros((400,1))\n",
    "    phi[0:300] = np.random.uniform(low = [0.0], high = [((55.0/180)*np.pi)],size = [300,1])\n",
    "    phi[300:400] = np.random.uniform(low = [((65.0/180)*np.pi)], high = [((90.0/180)*np.pi)],size = [100,1])\n",
    "\n",
    "    parms_obl = np.concatenate((bond_len,phi),axis = 1)\n",
    "    \n",
    "    #square  a1 = a2, phi = 90, Label = 1\n",
    "    parms_sq = np.zeros([400,3])\n",
    "    parms_sq[:,0] = np.random.uniform(low = 0.8, high = 2.0,size = 400)\n",
    "    parms_sq[:,1] = parms_sq[:,0]\n",
    "    parms_sq[:,2] = np.pi/2\n",
    "    \n",
    "    #Hexagonal a1 = a2, phi = 60 Label = 2\n",
    "    parms_hex = np.zeros([400,3])\n",
    "    parms_hex[:,0] = np.random.uniform(low = 0.8, high = 2.0,size = 400)\n",
    "    parms_hex[:,1] = parms_hex[:,0]\n",
    "    parms_hex[:,2] = (np.pi/3)\n",
    "    \n",
    "    #rectangular a1 != a2, phi = 90 Label =3\n",
    "    parms_rec = np.random.uniform(low = [0.8,0.8,(np.pi/2)], high = [2.0,2.2,(np.pi/2)],size = [800,3])\n",
    "    diff_rec = np.abs(((parms_rec[:,0]-parms_rec[:,1])*100)/parms_rec[:,1])\n",
    "    \n",
    "    parms_rec = parms_rec[diff_rec > 20.0,:]\n",
    "    parms_rec = parms_rec[0:400,:]\n",
    "    \n",
    "    \n",
    "    #centered  a1 = a2, phi != 90, Label = 4\n",
    "    bond_len = np.zeros([400,2])\n",
    "    bond_len[:,0] = np.random.uniform(low = 0.8, high = 2.0,size = 400)\n",
    "    bond_len[:,1] = bond_len[:,0]\n",
    "    phi = np.zeros((400,1))\n",
    "    phi[0:300] = np.random.uniform(low = [0.0], high = [((55.0/180)*np.pi)],size = [300,1])\n",
    "    phi[300:400] = np.random.uniform(low = [((65.0/180)*np.pi)], high = [((90.0/180)*np.pi)],size = [100,1])\n",
    "\n",
    "    parms_cen = np.concatenate((bond_len,phi),axis = 1)\n",
    "    \n",
    "    parameters = np.concatenate((parms_obl,parms_sq,parms_hex,parms_rec, parms_cen), axis=0 )\n",
    "    label = np.zeros([2000,1])\n",
    "    \n",
    "    label[401:801,0]=1\n",
    "    label[801:1201,0]=2\n",
    "    label[1201:1601,0]=3\n",
    "    label[1601:2001,0]=4\n",
    "    \n",
    "    parameters,label = shuffle(parameters, label)\n",
    "    parameters = parameters.T\n",
    "    \n",
    "    \n",
    "    for i in range(0,10):\n",
    "        parameters_list = parameters[:,i*100:(i+1)*100]\n",
    "        labels_list = label[i*100:(i+1)*100,0]\n",
    "        yield parameters_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_lWXobEyEWY"
   },
   "outputs": [],
   "source": [
    "def images_final(parms):\n",
    "  atom_pos = lattice(parms)\n",
    "  atom_pos_dis = distortions(atom_pos, parms)\n",
    "  image_atoms = atom_to_image(atom_pos_dis)\n",
    "  con_img = convolve_atomic_image(image_atoms)\n",
    "  crop_img = crop(con_img)\n",
    "  img_final = fft(crop_img)\n",
    "\n",
    "  return img_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wOYEjJdJISU"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Call .contiguous() before .view(). at /opt/conda/conda-bld/pytorch_1535493744281/work/aten/src/TH/generic/THTensor.cpp:237",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-75ad64fa5135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdata_ten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdata_ten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_ten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlabels_ten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Call .contiguous() before .view(). at /opt/conda/conda-bld/pytorch_1535493744281/work/aten/src/TH/generic/THTensor.cpp:237"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "a =  generate()\n",
    "parameters_list, labels_list = next(a)\n",
    "data = images_final(parameters_list)\n",
    "data = data.astype(np.float32)\n",
    "\n",
    "T = torchvision.transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "data_ten = T(data)\n",
    "data_ten = data_ten.view(100,1,128,128)\n",
    "\n",
    "labels_ten = torch.from_numpy(labels_list).long()\n",
    "\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(data_ten,labels_ten)\n",
    "val_dl = torch.utils.data.DataLoader(val_dataset,batch_size = 100)\n",
    "\n",
    "\n",
    "class myCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(myCNN,self).__init__()\n",
    "    self.cnn1 = nn.Conv2d(1,16,2)\n",
    "    self.cnn2 = nn.Conv2d(16,32,2)\n",
    "    self.cnn3 = nn.Conv2d(32,32,2)\n",
    "    \n",
    "    self.fc1 = nn.Linear(500000,128)\n",
    "    self.fc2 = nn.Linear(128,50)\n",
    "    self.fc3 = nn.Linear(50,10)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.dropout = nn.Dropout2d()\n",
    "    \n",
    "  \n",
    "  def forward(self,x):\n",
    "    n = x.size(0)\n",
    "    x = F.relu(self.cnn1(x))\n",
    "    x = F.relu(self.cnn2(x))\n",
    "    x = F.relu(self.cnn3(x))\n",
    "    x = x.view(n,-1)\n",
    "    \n",
    "    \n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.dropout(x)\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.dropout(x)\n",
    "    x = (self.fc3(x))\n",
    "    \n",
    "    return x\n",
    "  \n",
    "mycnn = myCNN().cuda()\n",
    "cec = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mycnn.parameters(),lr = 0.001)\n",
    "\n",
    "\n",
    "def validate(model,data):\n",
    "  # To get validation accuracy = (correct/total)*100.\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  for k,(images,labels) in enumerate(data):\n",
    "    images = Variable(images.cuda())\n",
    "    x = model(images)\n",
    "    value,pred = torch.max(x,1)\n",
    "    pred = pred.data.cpu()\n",
    "    total += x.size(0)\n",
    "    correct += torch.sum(pred == labels)\n",
    "  return correct*100./total\n",
    "\n",
    "for j in range(18):\n",
    "    parameters_list, labels_list = next(a)\n",
    "    data = images_final(parameters_list)\n",
    "    data = data.astype(np.float32)\n",
    "\n",
    "    T = torchvision.transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    data_ten = T(data)\n",
    "    data_ten = data_ten.view(100,1,128,128)\n",
    "\n",
    "    labels_ten = torch.from_numpy(labels_list).long()\n",
    "\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(data_ten,labels_ten)\n",
    "    train_dl = torch.utils.data.DataLoader(train_dataset,batch_size = 20)\n",
    "\n",
    "\n",
    "    for e in range(35):\n",
    "      for i,(images,labels) in enumerate(train_dl):\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        pred = mycnn(images)\n",
    "        loss = cec(pred,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 5 == 0:\n",
    "          accuracy = float(validate(mycnn,val_dl))\n",
    "          print('Epoch :',e+1,'Batch :',i+1,'Loss :',float(loss.data),'Accuracy :',accuracy,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MaKEVO494DzR"
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
