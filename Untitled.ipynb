{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--e E] [--b B] [--train TRAIN] [--val VAL]\n",
      "                             [--test TEST] [--dropout DROPOUT]\n",
      "                             [--kernel KERNEL] [--optimizer OPTIMIZER]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--momentum MOMENTUM]\n",
      "                             [--weight_decay WEIGHT_DECAY] [--seed SEED]\n",
      "                             [--train_data_dir TRAIN_DATA_DIR]\n",
      "                             [--val_data_dir VAL_DATA_DIR]\n",
      "                             [--test_data_dir TEST_DATA_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1003/jupyter/kernel-28bedbd7-6335-4bd8-8209-b738528bd903.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# A Python script to construct the model and train it\n",
    "# Code for argument passing has to be written. \n",
    "# Hopefully this works out well enough that I dont have to spend too much time trying to fix issues. \n",
    "\n",
    "import argparse\n",
    "from keras.utils import plot_model \n",
    "from Cordconv import CordConv as base_model\n",
    "from dataLoader import data_loader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "#args = parser.parse_args()\n",
    "# Hyperparameters\n",
    "parser.add_argument('--e', help=\"The number of epochs\", type = int, default = 10)\n",
    "parser.add_argument('--b', help=\"Batch size\", type = int, default = 128)\n",
    "parser.add_argument('--train', help = \"The portion of data for training\", type= float, default = 0.7)\n",
    "parser.add_argument('--val', help = \"The portion of data for validation\", type = float, default = 0.15)\n",
    "parser.add_argument('--test', help = \"The portion of data for testing\", type = float, default = 0.15)\n",
    "parser.add_argument('--dropout', help = \"Whether to include dropout\", type = float, default = .2)\n",
    "parser.add_argument('--kernel', help = \"Kernel size\", type = int, default = 5)\n",
    "\n",
    "# Optimizer set up\n",
    "parser.add_argument('--optimizer', help=\"The optimizer to use\", type = str, default = 'adam')\n",
    "parser.add_argument('--learning_rate', help=\"the learning rate\", type = float, default = '0.0001')\n",
    "parser.add_argument('--momentum', help = \"The momentum\", type = float, default = 0.9)\n",
    "parser.add_argument('--weight_decay', help = \"Weight decay factor\", type = float, default = 0)\n",
    "parser.add_argument('--seed', help = \"Seed for random initializations\", type = float, default = 43)\n",
    "\n",
    "# Data flow\n",
    "parser.add_argument('--train_data_dir', help = \"Directory containing training data\", type = str, default = 'data/train')\n",
    "parser.add_argument('--val_data_dir', help = \"Directory for validation data\", type =str, default = \"data/val\")\n",
    "parser.add_argument('--test_data_dir', help = \"Directory for test data\", type = str, default = \"data/test\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Build model\n",
    "model = base_model(learning_rate = args.learning_rate, dropout = args.dropout, kernel_size = args.kernel)\n",
    "CNN = model.build_model()\n",
    "#filepath = \"weights-imp-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "filepath=\"weights_ve_res-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "# Load the data\n",
    "# CNN.load_weights(\"weights_ve_res-improvement-32-0.83.hdf5\")\n",
    "\n",
    "dataLoader = data_loader(args.train_data_dir, args.val_data_dir, args.test_data_dir)\n",
    "#dataLoader = data_loader('./data/train/', './data/val/', './data/test/')\n",
    "train_generator, validation_generator, test_generator = dataLoader.load_images()\n",
    "checkpoint_1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint_2 =  TensorBoard(log_dir='logs/{}', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint_2,checkpoint_1]\n",
    "\n",
    "\n",
    "\n",
    "#X_train, Y_train, X_test, Y_test, val_data = dataLoader.load_images()\n",
    "#print('Training data size', len(X_train))\n",
    "start = time.time()\n",
    "CNN.fit_generator(train_generator, steps_per_epoch =100, epochs = 100,callbacks=callbacks_list,validation_data  = validation_generator,validation_steps = 2 )\n",
    "print('--------Test data--------')\n",
    "\n",
    "x = CNN.evaluate_generator(test_generator, steps = 10, verbose = 1)\n",
    "print(x)\n",
    "end = time.time()\n",
    "print('Time taken: ', str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
